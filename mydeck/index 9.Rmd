---
title       : "Week 9: Model Performance and Feature Engineering"
subtitle    : '`r format(Sys.Date(), "%m/%d/%Y")`'
author      : "Jake Campbell"
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : mathjax            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

```{r, echo=F, message=F, warning=F}
library(caret)
library(pROC)
library(Metrics)
library(MASS)
library(tidyverse)

data("mpg")
data("biopsy")

mpg_model <- lm(hwy ~ displ + cyl + drv, data = mpg)
biopsy_model <- glm(class ~ V1 + V5 + V7, data = biopsy, family = "binomial")

# For linear regression, we can look at resdiuals to determine
# model performance
actual_mpg <- mpg$hwy
predicted_mpg <- predict(mpg_model)

actual_biopsy_class <- factor(biopsy$class)
predicted_biopsy_class <- factor(ifelse(
  predict(biopsy_model, type = "response") >= .5, "malignant", "benign"))
predicted_biopsy_probability <- predict(biopsy_model, type = "response")
```

# Measuring Model Performance

- The main purpose of most predictive modeling problems is to make predictions
- It makes sense, than, to measure model performance off of our predictions
  + Using residuals with linear regression and predicted classes and probabilities with logistic regression

---

# Remember Overfitting

- We'll be looking at predictions made on our training data today
- These predictions are likely going to be better than we should actually expect
  + Our model has seen this data before
- Next class, we'll be going over cross-validation, allowing us to see how our model performs on new data

---

# Linear Regression Model Metrics

- MAE: Mean Absolute Error
  + Takes the average absolute value of your residuals
```{r}
mae(actual = actual_mpg, predicted = predicted_mpg)
```

---

# Linear Regression Model Metrics

- MSE: Mean Square Error
  + Takes the average of squared residuals
- RMSE: Root Mean Square Error
  + Takes the square root of the MSE
```{r}
mse(actual = actual_mpg, predicted = predicted_mpg)
rmse(actual = actual_mpg, predicted = predicted_mpg)
```

---

```{r, echo=F, message=F, warning=F}
actual_biopsy_class <- factor(biopsy$class)
predicted_biopsy_class <- factor(ifelse(
  predict(biopsy_model, type = "response") >= .5, "malignant", "benign"))
predicted_biopsy_probability <- predict(biopsy_model, type = "response")

cm_biopsy <- xtabs(~ predicted_biopsy_class + actual_biopsy_class)

set.seed(1234)
mpg <- mpg %>%
  mutate(price = paste("$", round(rnorm(n = 234, mean = 20000, sd = 5000),
                                  digits = 0), sep = "")) %>%
  mutate(production_date = sample(seq(as.Date('1998/01/01'),
                                      as.Date('2008/12/31'), by="day"), 234))
```

# Logistic Regression Metrics

- Accuracy: $$\frac{All_correct_predictions}{All_possible_predictions}$$
- True Positives and Negatives: predictions that were correct from the positive and negative class, respectively
- False Positives and Negatives: predictions that were predicted in the wrong class(positive and negative respectively)

---

# Logistic Regression Metrics

- Sensitivity: $$\frac{TP}{TP + FN}$$
  + Accuracy rate of the positive class
```{r}
sensitivity(reference = actual_biopsy_class, data = predicted_biopsy_class,
            positive = "malignant")
```

---

# Logistic Regression Metrics

- Specificity: $$\frac{TN}{TN + FP}$$
  + Accuracy rate of the negative class
```{r}
specificity(reference = actual_biopsy_class, data = predicted_biopsy_class,
            negative = "benign")
```

---

# ROC and AUC

- ROC (Receiver Operating Characteristic) is a curve that plots the sensitivity and 1 - specificity at different predictive threshholds
  + By threshhold, we mean how to split our predictive probabilities into classes
  + Raising the threshhold increases our specificity and vice versa
- AUC (Area Under the Curve) is the area under the ROC curve
  + The closer AUC is to `1`, the better the classifier

---

# ROC and AUC

```{r, warning=F, message=F, fig.align='center'}
roc(response = actual_biopsy_class,
    predictor = predicted_biopsy_probability, plot = T)
```

---

# Feature Engineering

- A lot of the time, predictors are not laid out in front of us
- We have to transform some features to have them actually be useful in our model
  + Ex: a specific date doesn't hold much predictive power, but a month could

---

# Splitting Text

- `str_split` can be used to split text off of some pattern
  + `str_split_fixed` returns a matrix instead of a list
```{r}
transmission <- str_split_fixed(string = mpg$trans, pattern = "[(]", n = 2)
head(transmission)
```

---

# Replacing Text

- `gsub` can be used to replace a certain character in text with another
  + A lot of times, you may want to replace a character with nothing, in which case, you would replace it with `""`
```{r}
mpg <- mpg %>%
  mutate(price = gsub(pattern = "[$]", replacement = "", price)) %>%
  mutate(price = as.numeric(price))
```

---

# Dealing with Dates

- Dates are a specific data type in R
- We can use `as.Date` to turn something into a date
  + We need to specify the `format` argument to say what order the date is in
  + By default, the format is `"%Y-%m-%d"` for full year, numeric month, and numeric day
  + For example: `2019-12-01`


---

# Dealing with Dates

- `format` is also a function we can use to pull a specific part of a date object out
```{r}
mpg <- mpg %>%
  mutate(month = format(x = production_date, "%m"),
         year = format(x = production_date, "%Y"),
         day = format(x = production_date, "%d"))
```